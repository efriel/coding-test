"""
Gemini AI Service Module

This module provides functionality for interacting with the Gemini AI model. 
It sends prompts to the Gemini API endpoint and retrieves responses.

Purpose:
- To integrate with the Gemini AI model hosted by Google.
- To process prompts and return responses generated by the Gemini AI model.

Dependencies:
- The `requests` library is used to make HTTP requests to the Gemini API.
- The `settings` module provides the `GEMINI_API_KEY` configuration.

Functions:
- ask_gemini: Sends a prompt to the Gemini AI model and retrieves the response.

Integration:
- This service is used by the `ai_services.py` module to route prompts to the Gemini AI model.
"""

import requests
from config.settings import settings

def ask_gemini(prompt: str) -> str:
    """
    Sends a prompt to the Gemini AI model and retrieves the response.

    Parameters:
    - prompt (str): The input prompt to be processed by the Gemini AI model.

    Returns:
    - str: The response generated by the Gemini AI model, or a fallback message if no response is available.

    Example Usage:
    >>> response = ask_gemini("What is the weather today?")
    >>> print(response)
    """
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
    headers = {"Content-Type": "application/json"}
    params = {"key": settings.GEMINI_API_KEY}  # API key for authentication
    data = {"contents": [{"parts": [{"text": prompt}]}]}  # Request payload

    try:
        response = requests.post(url, headers=headers, params=params, json=data)
        response.raise_for_status()  # Raise an error for HTTP status codes >= 400
        return response.json().get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "No response")
    except requests.exceptions.RequestException as e:
        return f"Error communicating with the Gemini AI model: {str(e)}"
